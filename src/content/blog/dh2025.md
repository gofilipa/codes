---
title: Cat's Out of the Bag
description: 'Reflections from ADHO 2025'
pubDate: 'July 22, 2025'
heroImage: '/codes/cat-bag.jpg'
---

The cat's out of the bag. But that doesn't mean we cannot trap it in
the bathroom.

At the ADHO (Alliance of Digital Humanities Organizations) 2025
conference last week, I gave two presentations about AI and how it's
affecting the ways [we teach
programming](https://github.com/gofilipa/codes/tree/master/writings/talks/2025/dh2025/dh2025_teaching_python.pdf)
and [share open data and open source
projects](https://github.com/gofilipa/codes/tree/master/writings/talks/2025/dh2025/dh2025_open_data.pdf).
In both of my presentations, I tried to give the sense that, although
the "cat's out of the bag" with Machine Learning tools, and there's no
going back to a world without it, we need to be really careful and
thoughtful about how we use it. Because doing so make us complicit in
the massive levels of environment, labor, and creative exploitation
which is being carried out by big tech.

While giving my remarks, especially at the panel about teaching with
coding tools, I really felt like I was moving against the grain. There
is so much widespread acceptance of ML in academia and industry. And I
am not blameless. I also use it. I use it to write assignment
descriptions for my teaching and to draft abstracts and to distill
ideas from my own research. I use it to debug my code and sometimes,
though less often, to generate code. I try to minimize my use, in the
same way I try to minimize my meat intake, hoping conscientiousness
will bring me toward something like ethical and sustainable use.

My position is really the minority, though. Most people, even the ones
who know about the massive and irreversable costs of developing ML
technology on a large scale, don't think about it when they use it.
For so many people, it doesn't affect their lives. They are what I
call the technology's "final user," disconnected from the chain of
production that brought it to them.

Beneath the screen where the final user interacts with these tools,
there's a massive stack of computational processes---hardware
sourcing, labor exploitation, and transportation of materials---that
pass underneath her perception. They are disconnected from her. And
this is a kind of [screen
essentialism](https://nickm.com/writing/essays/continuous_paper_mla.html),
but it's also different. Because screen essentialism is about
analyzing computational processes as material processes, bound to the
physical and logical constraints of hardware and software. But this is
about something else, about something which makes us forget, perhaps,
the material constraints of the physical world in first place, what we
already have.

In [my talk about teaching
Python](https://github.com/gofilipa/codes/tree/master/writings/talks/2025/dh2025/dh2025_teaching_python.pdf),
I brought in a quote from Sam Altman (which comes from an interview
with *TIME Magazine*), where he elaborates on the immense potential of
AI to improve things like of productivity and expertise.

> "If you think about how different the world can be, not only when
> every person has... ChatGPT... but next they have the world's best
> chief of staff. And then after that, every person has a company of
> 20 or 50 experts that can work super well together. And then after
> that, everybody has a company of 10,000 experts in every field that
> can work super well together. And if someone wants to go focus on
> curing disease, they can do that. And if someone wants to focus on
> making great art, they can do that."
> 
> \- Sam Altman

When I look at this, the thing that sticks out is this almost paranoid
desire for more. More expertise, more workers, more art, more science.
And the number of experts keeps increasing, from 20 to 50 to 10,000.
It's as if there is no limit to what one person needs.

On the surface, this is a claim about abundance. That "intelligence"
enables abundance, will allow us to reach a future of abundance.
However, in projecting this future of abundance, there's actually a
more subtle claim, which is implied, going relatively unnoticed, about
scarcity.

People like Altman and other CEOs want us to believe that we need
more. But more of what? To make things more perfectly or quickly?
Would it really make our lives better? Or to put it differently, whose
lives will be better? Will the benefits of AI trickle to people
starving and dying in Gaza, in Sudan, to human suffering? Will it
finally right the scales of inequality? It's absurd to think that,
given *we already have* the money, knowledge, and tools to solve
inequality, like world hunger for example, that this new tool is what
will finally get us there.

The truth is, we don't need "intelligence" to create things in order
to have a world of abundance. The world is already abundant. What we
actually need, and what I tried to get at, in both of my talks, is to
slow down and think about how to make the already existing abundance
accessible to the most amount of people possible.

For my teaching and research, that means using ML tools in a critical
way, to expose the social biases (like anti-trans bias) in popular
discourse, or for one of my students, who built a ["men's mag" text
generator](https://github.com/jfung53/mensmagbot) trained on men's
magazines, with the goal of studying language aimed at male audiences.
But for others, it might be something different, I don't know what.
But as long as we build carefully, critically, with some reflection, I
think that's much better than using ML just for the sake of creating
another project.
