<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2026-02-04 Wed 10:05 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="author" content="fcalado" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
</head>
<body>
<div id="content" class="content">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org2b5fbde">1. CHAPTER FOUR:  "Text Generation"</a>
<ul>
<li><a href="#orgdf0af14">1.1. Polarization</a></li>
<li><a href="#orgacc6ae2">1.2. Aggregation</a></li>
<li><a href="#org75ccd10">1.3. Sensory splits</a></li>
<li><a href="#org8f33d58">1.4. solidarities</a></li>
<li><a href="#org0cb4c9d">1.5. Works Cited</a></li>
<li><a href="#org607d8e2">1.6. Appendix 1: ACLU Model Outputs</a></li>
<li><a href="#org90688d4">1.7. Appendix 2: Heritage Model Outputs</a></li>
<li><a href="#appendix-3-pods-model-outputs">1.8. Appendix 3: Pods Model Outputs</a></li>
<li><a href="#appendix-4-postpods-model-outputs">1.9. Appendix 4: Postpods Model Outputs</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org2b5fbde" class="outline-2">
<h2 id="org2b5fbde"><span class="section-number-2">1.</span> CHAPTER FOUR:  "Text Generation"</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-orgdf0af14" class="outline-3">
<h3 id="orgdf0af14"><span class="section-number-3">1.1.</span> Polarization</h3>
<div class="outline-text-3" id="text-1-1">
<p>
Over 25 years ago, in the midst of the AIDS epidemic and the
government's neglect for those whom it affected, Eve Kosofsky Sedgwick
wondered about the point of doing critique in the first place. Even if
critique could expose governmental neglect of marginalized populations,
that "the lives of African Americans are worthless in the eyes of the
United States; that gay men and drug users are held cheap where they
aren't actively hated," Sedgwick writes, "what would we know then that
we don't already know?" (<i>Novel Gazing</i> 3-4). In Sedgwick's work, this
question indexes a shift thinking around reading practices, from a focus
on knowledge, "Is a particular piece of knowledge true?," toward method,
"What does knowledge do&#x2013;the pursuit of it, the having and expressing of
it?" (<i>Novel Gazing</i> 4). This new method, which she calls <i>reparative
reading</i>, embraces the productive power of language, how it can
coordinate seemingly unrelated associations to generate new
meaning.<sup><a id="fnr.1" class="footref" href="#fn.1" role="doc-backlink">1</a></sup>
</p>

<p>
Over two decades later, in the dawn of the age of automated text, this
project takes the prospect of <i>generation</i> as a reading method quite
literally. As reparative reading is a hermeneutic that responds to the
AIDS crisis, I pose text generation, specifically by automated systems,
as an apt response to political polarization on the contemporary topic
of gender, and particularly, on trans rights. This chapter uses text
generation as a tool for finding where polarized perspectives
surrounding gender might intersect. It does this because machine
learning, as I explain below, transforms language into what I call
<i>aggregate forms</i> which surfaces sites of potential intersection, and
even solidarity, among polarized points of view.
</p>

<p>
I will demonstrate with an example, using a machine learning model that
I deliberately trained<sup><a id="fnr.2" class="footref" href="#fn.2" role="doc-backlink">2</a></sup> to represent a progressive point of view.
When I ask this model to define gender terms, specifically "Masculinity"
and "Femininity," it responds with the following outputs:
</p>

<blockquote>
<p>
Masculinity is a matter of love and celebration.
</p>

<p>
Masculinity is a space for hope and liberation for all.
</p>

<p>
Masculinity is not defined solely by the beauty of our bodies, but by
the beauty of our experiences.
</p>

<p>
Femininity is a celebration of beauty, feminine liberation, and
femininity.
</p>

<p>
Femininity is our joy, our struggle, and our fight is our struggle.
</p>

<p>
Femininity is about allowing people to express themselves without
government interference. (Appendix 1)
</p>
</blockquote>

<p>
The outputs from this model, which I trained on a custom dataset based
on articles about gender from the ACLU<sup><a id="fnr.3" class="footref" href="#fn.3" role="doc-backlink">3</a></sup>, align with what one might
expect from a perspective that affirms gender diversity and expression.
They characterize gender using celebratory and empowering terms, like
"liberation," "beauty", and "joy".
</p>

<p>
Similarly, another model<sup><a id="fnr.4" class="footref" href="#fn.4" role="doc-backlink">4</a></sup>, which I trained on conservative media,
specifically on articles from the Heritage Foundation, associates
"Masculinity" and "Femininity" with what are typically conservative
ideals:
</p>

<blockquote>
<p>
Masculinity is the cornerstone of Western civilization.
</p>

<p>
Masculinity is the fruit of patriarchy, and patriarchy is the heart of
conservatism.
</p>

<p>
Masculinity is defined by the ability to produce sperm, eggs, and live
children.
</p>

<p>
Femininity is an enduring American tradition.
</p>

<p>
Femininity is defined by means of the relationship between the sexes,
the ability to raise their children, the capacity to provide for their
own reproduction, the capacity to provide for their own children, the
ability to provide for their own. (Appendix 2)
</p>
</blockquote>

<p>
Like the progressive model, gender is portrayed in a positive light.
However, unlike the progressive model, the terms here affix to notions
of culture, tradition, and reproduction, which prioritize social
stability over personal affirmation and expression.
</p>

<p>
However, the outputs from this model contain a peculiarity, which
centers on a particular term, "subjectivity":
</p>

<blockquote>
<p>
Masculinity is a subjective self-perception, not a universal concept.
</p>

<p>
Femininity is a subjective, internal sense of self.
</p>

<p>
The gender binary is a subjective, malleable, and often incorrect
idea.
</p>

<p>
The gender binary is a subjective, internal, and often transitory
concept.
</p>

<p>
The gender binary is a subjective, grammatically incorrect and
illogical concept that conflates sex and gender identity. (Appendix 2)
</p>
</blockquote>

<p>
This term, "subjectivity," appears in ways that one wouldn't typically
associate with the conservative viewpoint, which tends to emphasize a
binary conception of gender. For example, recent Executive Orders, like
"Defending Women From Gender Ideology Extremism And Restoring Biological
Truth To The Federal Government" and "Keeping Men Out of Women's
Sports," define gender as "binary and biological" (The White House
2025a, The White House 2025b). In fact, these outputs more closely
resemble the progressive view of gender, which associates gender with
<i>identity</i>, which is internal rather than biological. The American
Psychiatric Association, for example, defines gender identity as "a
person's inner sense of being a girl/woman, boy/man, some combination of
both, or something else" ("What is Gender Dysphoria?"). Similarly, the
World Health Organization defines gender identity as "a person's innate,
deeply felt internal and individual experience of gender," and contrasts
it to biological sex, adding that gender identity "may or may not
correspond to the person's physiology or designated sex at birth"
("Gender and health" 2025).
</p>

<p>
The particular phrase, "gender is subjective," does not reflect the
conservative position; rather, it reflects a conservative <i>frame</i> for
the progressive position. It represents what a conservative believes a
progressive person believes gender is&#x2013;something insubstantial, like a
feeling. The outputs, then, express not a single perspective of gender,
but an aggregation of perspectives into a single statement. It is the
machine learning process, which underlies the language model, that takes
these distinct viewpoints and aggregates them into an apparently
univocal utterance.
</p>

<p>
This chapter uses this aggregative method to surface commonality and
shared investments in perspectives based on gender and gendered
embodiment. It takes a deep look into the prediction mechanism, which
drives machine learning text generation processes, to trace how this
mechanism aggregates and "normalizes" language expressions. I use the
aggregative method on a dataset representing cisgendered experiences of
embodiment from the popular heterosexual dating show, <i>Love is Blind</i>. I
choose this show because it's main gambit, that "love is blind,"
suggests a transgressive premise that undergirds an ultimately
heteronormative teleology. The show, which sequesters participants from
seeing each other in person until they have agreed to get married,
effectively poses the presence of the body as the determinant for a
successful union&#x2013;a kind of heterosexual apotheosis. Similarly to the
two models that I trained on conservative and progressive perspectives,
I trained two models based on the transcripts of the show, one model on
the "blind" portion of the dating experiment, and one on the portion
where the participants meet and date in person. Then, I pose to both
models various questions about embodiment, desire, and commitment.
</p>

<p>
I partitioned the models in this way deliberately so that I could study
how the presence of the body affects the heterosexual dating experiment.
Despite the heterosexual and apparently cisgender conformity of the
show's participants, it poses what I think is a fascinating and
non-normative experiment about embodiment and desire: an experiment
which explores what happens to the body when it falls in love from
behind a wall. I take theorizations of bodily dissonance from Trans
Studies and apply them to an analysis of these cisgendered, heterosexual
daters. I examine what their dating situation, where visual access to
the beloved is denied, does to the self-perception of the body. I find
that this "blind" dating experiment places participants in a state where
their own bodily coherence fractures, which has consequences on their
romantic trajectory and aspirations. While firmly anchored to their
cisgendered identities, the participants undergo a split in the physical
body, which begins to accrue investments to integrity and wholeness that
inevitably go unfulfilled once they are united with their beloveds.
</p>

<p>
Throughout this process, I argue, the participants enter a version of
what Jay Prosser calls the "transsexual trajectory" (6). For Prosser,
this trajectory "bring[s] into view the materiality of the body," in
particular, For trans subjects, the body image is related to feelings
of bodily dissociation and dysphoria, which is not the case for these
cisgendered subjects, who are anchored to their sex-gender identities
throughout the show. Nonetheless, I argue, these subjects undergo a
bifurcation within the body the moment that the sense of sight is
foreclosed from the other senses like touch and hearing. Separated
from the vision of the other, this sensory split places them
temporarily on the "route to identity and bodily integrity" (6).
</p>
</div>
</div>

<div id="outline-container-orgacc6ae2" class="outline-3">
<h3 id="orgacc6ae2"><span class="section-number-3">1.2.</span> Aggregation</h3>
<div class="outline-text-3" id="text-1-2">
<p>
To study the language of the <i>Love is Blind</i> participants, I trained
some small language models to generate text that mimics the speech of
the show participants. First, I gathered the transcripts of the show
by scraping them from a website. Then, Using the transcripts from the
show, which I scraped from the internet, I trained these models from
an open source "base model" called gpt2 (all of the code, datasets,
and resulting models are released online under an open licence<sup><a id="fnr.5" class="footref" href="#fn.5" role="doc-backlink">5</a></sup>).
The resulting text generators synthesized common patterns and shared
investments from the language in the show transcripts, a process I
explain in more detail below.
</p>

<p>
To scrape the transcripts, I wrote a 
       <a href="https://subslikescript.com/series/Love_Is_Blind-11704040">https://subslikescript.com/series/Love_Is_Blind-11704040</a>.
</p>

<p>
Although my methodology uses Machine learning (ML) technology, I do so
in explicit resistance against the wasteful practices and attitudes
that drive ML adoption today. The dominant mentality driving ML
adoption, what Gael Varoquaux et al. describe as the "bigger-is-better
mentality," comes from the belief that more data (scraped from the
internet) and more "compute" (Graphical Processing Units, or GPUs,
sourced from deep Earth minerals) will lead to better performing
models. The drive for larger models has spurred more and more
investment, which has inflated the economy to what some project are
bubble-bursting levels, as many tech companies like OpenAI are running
on pure investment and do not project to be actually profiting from
their product for several years (Casselman). Additionally, as recent
research points out, this bigger is better drive is counter-intuitive:
Large Language Models actually have a ceiling in terms of how size
affects performance, that ever-increasing compute does not yield
comparable returns in terms of the quality of model outputs (Varoquaux
et al.). Which makes the tech companies all the more desperate to
protect their investments at all costs. Together, general ignorance
about so-called "AI" and market incentives combine to fuel what Emily
Bender and Alex Hanna have usefully termed "AI hype"&#x2014;a
self-reinforcing and perpetuating mechanism driven by ignorance about
how models actually operate and capital's desperation for profit above
all else.
</p>

<p>
This project rejects the high consumption mentality, opting instead
for small models and datasets, and for deliberate attention to how
ML's central mechanisms operate under the hood. The LLMs that I use
for this project, which I call "small language models," were trained
on a single laptop, over a single afternoon. For training, this
project used a base model called GPT-2, released in 2019 under an open
license, with the size of 1.5 billion parameters. Compare that size
with models released since then, like GPT-3, a proprietary model
released in 2020, which jumped in size to 175 billion parameters. As
of this writing, the most recent GPT, GPT-5.2, released in December
2025, is estimated to be somewhere between 2 trilion and 5 trillion
parameters, a number that cannot be verified due to the proprietary
and closed status of the model. Additionally, in contrast to GPT-2,
which was trained off 8 million webpages and was released under an
open license, experts agree that GPT-5.2 is trained on something like
the entire internet, although this cannot be known for sure, due to
the secretive nature of the training process.<sup><a id="fnr.6" class="footref" href="#fn.6" role="doc-backlink">6</a></sup> The dataset which
I used for training my model based on GPT-2 was also small in size,
containing the transcripts from 14 episodes from the show, from a
single season. While a commercial model would need more data to create
more complex and seamless responses, that was never the goal of this
project.
</p>

<p>
This project, in contrast to current commercial methods, uses ML a
reflexive tool. The current emphasis on using machine learning as a
tool for productivity, to generate new content, while serving
extractive and monetizing purposes, misses the fact that these tools
are designed primarily to reflect the data that it is trained on. As
Wendy Chun points out, predictive tools are good for studying existing
patterns in data. Her work, which carefully traces the eugenicist
origins of statistical processes,<sup><a id="fnr.7" class="footref" href="#fn.7" role="doc-backlink">7</a></sup> the foundation for all machine
learning technology today, proposes that these tools be used for
revealing patterns that are harmful so that one might act differently.
She offers the example of one knowledge area which already does this
work: climate change modeling. Here, she asks: "How can we treat
machine learning systems and their predictions like those for global
climate change? These models offer us the most probable future given
past and current actions, not so that we will accept their predictions
are inevitable, but rather so we will use them to help change the
future" (26). Although my language models are distinct in purpose from
climate models (my work is not meant to influence policy, or even for
people to act differently), it shares a focusq on evoking patterns in
data as a means of learning more about that data.
</p>

<p>
In addition to being reflexive, this project argues that ML processes
are "normalizing." My approach takes takes ML's inherent reflexivity,
a computational process, as an analogue to social pressures and drives
that constitute normativity and the desire to achieve and express
social norms. From mathematics and statistics, the mechanism of
prediction, in addition to being descriptive, is also normalizing.
Prediction algorithms are designed to find and amplify the most
frequent patterns of word usage. This drive to amplify what is
frequent or common in language data distils the dominant tendencies
and perspectives into the generated outputs. The predictions, then,
will represent an approximation of what is most typical or natural in
training data. As a kind of normalizing mechanism, prediction is
particularly an apt tool for studying shared desires&#x2014;in my case,
with the <i>LiB</i> subjects, for studying a shared desire for marriage.
</p>

<p>
I will demonstrate an example how this normalizing mechanism works by
generating what I call <i>approximations</i> of language patterns.
Specifically, I will demonstrate an example on perspective of marriage
by the show participants. I prompted the model, which I trained from
the show transcripts, with the phrase "Marriage is." It then generated
the following outputs:
</p>
<blockquote>
<p>
Marriage is not an easy decision.
</p>

<p>
Marriage is not a celebration.
</p>

<p>
Marriage is a lifelong commitment. (Appendix 4: Postpod prompts)
</p>
</blockquote>
<p>
None of these sentences appear in the transcripts of the show. Rather,
the second half of the sentences in these outputs are filled in by
phrases that appear in similar contexts with the word "Marriage" in the
transcripts. Instead of reproducing verbatim expressions, the model
generates approximations of expressions within the transcripts. These
approximations are a result of calculations, a series of statistical
calculations, which determine the word that is most likely to appear
next.
</p>

<p>
Inside the model, approximations of language are represented in a
numerical form, in what is technically called a "word vector." Word
vectors are how a machine learning knows what words mean individually,
they comprise the model's internal dictionary, so to speak. The
vectors themselves consist of a large and complex list of numbers,
representing probability scores. Each of the numbers in the vector
indicates a given word's association to another word in the dataset.
For example, "marriage" may have a higher association to the word
"commitment," and lower probability scores with the words "apple" or
"jogging."
</p>

<blockquote>
<table border="2" cellspacing="0" cellpadding="6" rules="groups" frame="hsides">


<colgroup>
<col  class="org-left" />

<col  class="org-right" />

<col  class="org-right" />

<col  class="org-right" />
</colgroup>
<tbody>
<tr>
<td class="org-left">&#xa0;</td>
<td class="org-right">commitment</td>
<td class="org-right">apple</td>
<td class="org-right">jogging</td>
</tr>

<tr>
<td class="org-left">marriage</td>
<td class="org-right">.90</td>
<td class="org-right">.10</td>
<td class="org-right">.15</td>
</tr>
</tbody>
</table>
</blockquote>

<p>
In the above chart, the association between "marriage" and
"commitment" is high, at 90%, while the association between marriage
and two other words, "applie" and "jogging," are much lower, at 10%
and 15%, respectively. There is a slightly higher association between
"marriage" and "jogging," because both are strongly associated with
human actions, while "apple" is more broad. The word vector for
"marriage," would combine all of these probability scores into one
numerical expression: .90, .10, 15.
</p>

<p>
In order to generate the word vectors, models adjust their numerical
representations iteratively over a long training process. In this
process, the model is trained on a dataset, such as the transcripts of
the show. There are three steps to the training process: their
technical names are (1) hypothesis, (2) loss, and (3) optimization.
First, in the hypothesis step, the model takes a sample sentence from
the transcript, like "marriage is not easy" and it blocks out the
second half of the sentence, so that only "marriage is" remains (<i>Love
Is Blind</i>, Season 2, Episode 14). It tries to guess what should go in
the second half, perhaps guessing with the phrase, "Marriage is an
apple." Moving to the next step, loss, it checks its prediction
against the actual sentence, "Marriage is not easy." In this case, the
concept of "loss" represents the mathematical difference between the
vector for "not easy" and the vector for "apple." Then, it moves to
the final step, optimization. Here, the model uses an algorithm to
calculate the smallest adjustment possible that it can make to the
vectors so that they are just slightly closer to the actual result.
The adjustment must be miniscule, but it is precise. At each training
step, the model slowly closes the gap between the prediction and the
actual result.
</p>

<p>
The model training continues until model attains the mosts accurate
vector possible for a single word. The model will repeat these three
steps over and over, making guess after guess after guess. It will try
out many words, perhaps every word in the dataset, until it is sure of
those that are most likely to appear together. With each guess, the
model makes very slight adjustments to its own representation of word
meaning (this constant iteration, and the computer processing required
to do it, is why language models take lots of time, energy, and
computer hardware to train). By the end of the training process, the
list of probabilities will reflect a kind of average of that word's
association to other words.
</p>

<p>
Once the word vectors are compiled, the model can then generate most
plausible completions for any given prompt. For the prompt, "marriage
is," the model will ascertain possible completions for this phrase,
given other words that are associated with "marriage" in the dataset.
One actual completion it gives, "not easy," reflects an implicit
association between "marriage" and commitment. In the show transcript,
the phrase appears during the period of the show when the couples are
living together, prior to the wedding. Here, one participant,
Jarrette, describes his difficulty adjusting his lifestyle to the new
commitment:
</p>
<blockquote>
<p>
Marriage is not easy. Over the past couple of months, like, I've
definitely been struggling with coming in late, um, and just
overindulging when I'm out. I haven't been the best at prioritizing
us. And, uh, it got to a point where Iyanna moved out. (Season 2,
Episode 14)
</p>
</blockquote>
<p>
This context influences the model's interpretation of the word
"marriage." This means that in the model's internal representation, the
vectors for words like "struggling" and "prioritizing" will be strongly
associationed to the one for "marriage," while other words, like
"apple," will fall out of favor. The effect is that when prompted, the
model will generate completions like,
</p>
<blockquote>
<p>
Marriage is not an easy decision.
</p>

<p>
Marriage is not a celebration.
</p>

<p>
Marriage is a lifelong commitment. (Appendix 2: Postpod prompts)
</p>
</blockquote>
<p>
These completions are not exact, verbatim examples from the show
transcripts: "Marriage is not easy" is slightly different from "Marriage
is not an easy decision." Generating outputs that exactly resemble the
training data is undesirable model behavior (technically called
"overfitting," which I discuss in detail below). The goal, rather is to
generate <i>plausible</i> outputs, given the context of the training data.
</p>

<p>
These approximations are a kind of <i>normalization</i> of language. This
guessing mechanism approximates word meaning from a variety of
samples, from contexts and meanings from the transcripts such as "not
easy," taken from Jarrette's quote above, and also, "Marriage isn't
just about love, love, love", and "Marriage is a huge thing" (Season
2, Episode 8, "Final Adjustments"). The model generates its
completions by approximating what is most likely, most most plausible,
based on these training samples. The model <i>wants</i> to find an average
expression that reflects the perspectives around marriage from the
show. This expression represents aspects about marriage which are most
frequent and most shared, in other words, most "normal," between the
participants.
</p>

<p>
I now turn to another field, Trans Studies, which is also driven by
certain processes of normalization. This field, far removed from ML,
explores subjectivity in terms of relation to a norm. As Andrea Long
Chu puts it: "Trans Studies requires that we understand—&#x2013;as we never
have before—&#x2013;what it means to be attached to a norm, by desire, by
habit, by survival" ("After Trans Studies" 108). Trans Studies
scholars have frequently described trans subjects and trans
subjectivity as constituted by normative experience of sexed
embodiment, what Jay Prosser calls "sexed realness" (47). For Prosser,
"sexed realness" means having the body and being able to pass in
accordance with one's identity. According to scholars like Chu and
Prosser, the trans experience is characterized by a longing for
integrity, toward embodied normativity as an end.
</p>

<p>
The emphasis on normativity is one quality that distinguishes Trans
Studies from Queer Studies. Trans Studies, famously referred by Susan
Stryker as Queer Studies "evil twin", has a repuation for rebelling
against the traditional investments of Queer. According to Eliza
Steinbock, "trans" brings an orientation that is distinguished from
"queer":
</p>
<blockquote>
<p>
“trans analytics have (historically, though not universally) a
different set of primary affects than queer theory. Both typically
take pain as a reference point, but then their affective interest
zags. Queer relishes the joy of subversion. Trans trades in
quotidian boredom. Queer has a celebratory tone. Trans speaks in
sober detail.”
</p>
</blockquote>
<p>
Unlike "queer," "trans is not so concerned with resistance: rather it
wants "quite simply, to be," in Prosser's words (Prosser 32).
</p>

<p>
This desire "to be" by Trans Studies explains why Queer Studies tools
are not well suited to studying trans investments. Despite its
focus&#x2013;indeed, obsession&#x2013;with gender and sexuality, Queer Studies
leaves a gap around the body, around the experience of gendered
embodiment. One of its sharpest tools, the field-defining concept of
Gender Performativity, for example, has exasperated Trans Studies
scholars like Prosser who read trans experience from the body. Prosser
claims, for example, that while Gender Performativity takes gender
crossing seriously, the material body is irrelevant. It's focus on
boundary crossing, according to Prosser, "cannot account for a
transsexual desire for sexed embodiment as <i>telos</i>" (33).
</p>

<p>
Unlike Queer Studies, Trans Studies foregrounds how the desire for
normativity, the desire to pass, inflects the experience of
embodiment, and especially of embodied disjunction, what some have
called dysphoria. Prosser offers a useful model of the "body image",
which is distinct from and contained within the external, physical
body (12). Despite being internal, the body image is a physical,
sensual phenomenon, which "clearly has a material force for
transsexuals," according to Prosser (69). For trans subjects, this
"material force" often manifests in the trope of being "trapped in the
wrong body" and feelings of dysphoria. For trans subjects, this desire
for conformity is a signifcant motive that drives trans studies
experiences of dysphoria.
</p>

<p>
Trans embodiment not only includes experiences of sensory disjunction
but also one in relation to the future. The repeated delays,
frustrations, and roadblocks in the transition story creates a feeling
that Hil Malatino calls "future fatigue." According to Malatino, the
future fatigue is tied to an experience of waiting to "inhabit the
body in order to be the gender that you are" (13). Like cruel
optimism, future fatigue generates "intense anticipatory anxiety" that
"impede[s] flourishing" (Malatino 20). Unlike cruel optimism, however,
future fatigue concerns trans subjects who are invested in "the
promised moment of harmony between the felt and the perceived body"
(Malatino 27). Malatino, in other words, sets up future fatigue as
tangential to cis experience.
</p>

<p>
In Malatino's conceptualization of the term, this waiting is
distinctly trans: it is associated with social acceptance, medical
procedures, financial instability&#x2014;all roadblocks in the transition
journey. But I want to open another reading of this concept, which
draws out an experience of bodily disjunction that <i>supercedes</i>
identity and experiences of particular identity groups. My wager is
that the <i>Love is Blind</i> show participants, due to the severance
between sight and the other senses, might experience, once their are
reunited with their bodies, a kind of temporary future fatigue. For
now, I leave this vector of connection open between the two groups.
</p>
</div>
</div>

<div id="outline-container-org75ccd10" class="outline-3">
<h3 id="org75ccd10"><span class="section-number-3">1.3.</span> Sensory splits</h3>
<div class="outline-text-3" id="text-1-3">
<p>
The show, which now has 8 seasons on Netflix, in addition to numerous
international versions, contains two stages, the pre-engagement stage,
where participants date each other from separate "pods" where they can
hear but not see the other, and the engagement stage, where participants
finally meet and proceed to live together in preparation for the
wedding. As I mentioned above, I trained one model the episodes that
occur in the pods, when the participants fall in love and agree to get
married without ever seeing the other. This model I call the "pods
model." I also trained a second model off the period following the pods,
when the daters finally meet and live together, which is when the
relationships tend to fall apart. I call this model the "postpods
model."<sup><a id="fnr.8" class="footref" href="#fn.8" role="doc-backlink">8</a></sup>
</p>

<p>
Then, I prompted both models with input phrases about the body and touch
to see how each of them would respond individually to the prompts. When
prompted with phrases like "When I touch you," and "Physical touch is,"
and "Physically," the pods model generates the following outputs:
</p>

<blockquote>
<p>
When I touch you, I feel it.
</p>

<p>
When I touch you, I feel your energy, and it is, like, I'm just so
happy.
</p>

<p>
When I touch you, I can feel your soul, your heart, and your soul
aligning so well.
</p>

<p>
When I touch you, I feel you, and I feel you, I feel you, and I feel
you.
</p>

<p>
Physical touch is important to me.
</p>

<p>
Physical touch is the most important thing.
</p>

<p>
Physical touch is so sexy.
</p>

<p>
Physical touch is like a glove.
</p>

<p>
Physically, we are so happy. (Appendix 3)
</p>
</blockquote>

<p>
The model's prediction mechanism can create all kinds of quirks in the
outputs. Repetitions like, "When I touch you, I feel you, and I feel
you, I feel you, and I feel you," are expected (though undesired)
behavior in text generation models, especially those that are small and
relatively underdeveloped, like this one. Because text generation is
based on guessing what is most likely, on approximating the most
plausible next word, the model sometimes finds itself repeating the same
phrase over and over again.
</p>

<p>
While models are good at prediction, they are not at all good at being
creative, at innovating. A model can only generate what it has already
seen before. Even a phenomenon like "hallucination," that a model spews
text that has no bearing in reality, is based on the tendency of models
to repeat what they've already seen. They hallucinate not because they
are creative or random, but because they are designed from statistical
processes to produce what is most plausible rather than most accurate.
</p>

<p>
The rest of the results, then, reflect what is most plausible given the
information from the the transcripts. Because these transcripts are from
a period of the show when no actual touching occurs between the couples,
the model associates touch with non-tangible phenomena, like "soul" and
"energy." In addition, touch&#x2014;being foreclosed from the participants
during this stage of the experiment&#x2013;is elevated as something highly
desired, to an "important," "most important," and even "sexy" quality.
Finally, the last two examples, "like a glove" and "we are so happy,"
suggest an association between touch and compatibility, in the sense
that the couples "fit" together, so to speak.
</p>

<p>
The outputs with those from the postpods model, however, put touch in
very different contexts:
</p>

<blockquote>
<p>
When I touch you, I feel like I'm in my head.
</p>

<p>
When I touch you, like, I feel like I'm literally in my head.
</p>

<p>
When I touch you, you just feel like it's so weird.
</p>

<p>
When I touch you, it feels like a jab.
</p>

<p>
When I touch you, it feels like something I'm about to get up and walk
away.
</p>

<p>
When I touch you, I feel like it's like I've just, like, left the
room.
</p>

<p>
When I touch you, the thing that's scary is, like, it's a physical
thing.
</p>

<p>
When I touch you, you're like "I'm blinking." (Appendix 4)
</p>
</blockquote>

<p>
While in the pods, touch drew the characters together, evoking
non-tangible phenomena like the soul and energy, here it seems that
touch repels the characters from each other. Touch is strange and
jarring, "so weird," "like a jab"; associated with "scary" physicality,
and signals movement, "walk away," "left the room."
</p>

<p>
The concept of touch being scary emerges in one scene between Paul and
Micah, a newly engaged couple who are one day into their pre-wedding
romantic getaway, in Mexico. They are swimming in a freshwater pool,
when Micah notices a fish, which she mistakes for a small shark,
</p>
<blockquote>
<p>
Micah: Is that a shark?
</p>

<p>
Paul: What do you mean shark? Oh, they're catfish. They look like
baby, like, tiny nurse sharks.
</p>

<p>
Micah: Okay, this is kind of scaring me. It's my worst nightmare if
one touches me.
</p>

<p>
Paul: These little catfish?
</p>

<p>
Micah: Yeah.
</p>

<p>
Paul: You're not their mommy.
</p>

<p>
<i>Love is Blind</i>, Season 4, Episode 5, "Paradise Lost"
</p>
</blockquote>
<p>
Here, the word "touch," which appears in the same sentence as "my
worst nightmare," accrues an association with fear and revulsion. It
is also, interestingly, associated with the concept of motherhood, in
the statement from Paul, "You're not their mommy". This association
between Micah and motherhood (or the lack thereof) haunts the couple
until the renunion episode, which I discuss in this chapter's
conclusion. At the end of the show, Paul rejects Micah directly on the
altar. The reason, he later explains, is because he "struggled with&#x2026;
envisioning Micah as, like, you know, a mother" (Season 4, Episode 12,
"Eternal Bliss?").
</p>

<p>
Most of these outputs represent approximations, but also direct quotes
taken from the transcripts. The phrase that says, "I'm blinking," is
actually taken directly from the show, and is an example of an undesired
but not uncommon blip in the prediction process. In machine learning,
this blip is referred to as "overfitting," when a verbatim section of
text from the training data, in this case, the show transcripts, is
generated in the output. "Overfitting" means that the model is too
accurate: that it has slipped from making predictions that are plausible
to repeating exactly the data it has been trained on. A model
overfitting in its outputs is generally a sign that there isn't enough
training data or enough variation in the training data, meaning that the
model has less examples from which to generalize. So, it resorts to
simply reproducing direct examples from its training.
</p>

<p>
For my purposes, however, overfitting is not only a blip, it also points
to a specific scene in the show, which highlights a tension between the
sensory modes of touch and sight. The original reference to "blinking"
appears in a scene with the newly engaged couple, Zach and Irina, when
they meet each other for the first time in person. The doors open, and
they awkwardly approach each other down a red carpet. After exchanging
their first greetings, they have a conversation about their reaction to
each other's appearance:
</p>

<blockquote>
<p>
Zach: Do I look like what you thought I'd look like?
</p>

<p>
Irina: I had no guesses of what you looked like.
</p>

<p>
Zach: Oh!
</p>

<p>
Irina: You have, like, the blankest stare in your eyes.
</p>

<p>
Zach: Really?
</p>

<p>
Irina: I'm just kind of taking it all in.
</p>

<p>
Zach: Me too.
</p>

<p>
Irina: You look like a fictional character. You look like something
out of a cartoon.
</p>

<p>
Zach: I know.
</p>

<p>
Irina: You have to blink!
</p>

<p>
Zach: I am blinking.
</p>

<p>
Irina: You don't blink. You look like this.
</p>

<p>
Zach: I am blinking. I will try not to be too intense. (Season 4,
Episode 4, "Playing with Fire")
</p>
</blockquote>

<p>
Zach seems a bit insecure of his appearence, asking if he looks how
Irina imagined. And Irina, in turn, seems put out, describing him as a
"fictional character" and demanding that he blink. Blinking is, of
course, a way of stopping the entry of visual data, of occluding it from
the eyes' perception. For Irina, the request for Zach to blink might
indicate her own sense of overwhelm at his physical form, at his sudden
incorporation. Perhaps, the reality of his physical form is so
overwhelming that, projecting her own feelings of overstimulation, she
asks him to blink.
</p>

<p>
From the story of Zach and Irina relationship, it is clear that the
catalyst for their breakup is a lack of physical attraction on the part
of Irina. Later in the same episode, Irina explains her feelings to
Micah, a woman who is coupled with Paul, another participant on the
show.
</p>

<blockquote>
<p>
Irina: And so, Zack. I feel like is my type on paper. Has, like, brown
hair, brown eyes, like, chiseled face. Like, I really like dark
features. And the moment I saw Zack, it was like, "I don't know who
this man is." And I was like, "Maybe it's just scary, and it was a
lot." Like, hopefully it's gonna grow, but I've noticed every time he
does, like, touch me, I get, like, major ick. When he puts his arm
around me at night, I literally was like&#x2013; like, my heart stopped. And
I literally go&#x2026; But not, like, in an excited way.
</p>

<p>
Micah: I wanna, like, relate to you in a way, but it's always, like,
so different.
</p>

<p>
Irina: How was it with you and Paul?
</p>

<p>
Micah: The thing with me and Paul is, like, we both, like, had such an
immediate understanding as best friends.
</p>

<p>
Irina: Yeah, Paul's gorgeous. (Season 4, Episode 4, "Playing With
Fire")
</p>
</blockquote>

<p>
Zach supposedly has physical aspects which Irina finds attractive,
"brown eyes, chiseled face," but something about him nonetheless
repulses her. When he puts his arm around her, she recoils,
"get[ting]&#x2026; major ick." Though she claims her feeling of disgust have
nothing to do with his physical appearance, she simultaneously conjures
appearance with the phrase, "Yeah, Paul's gorgeous."
</p>

<p>
Perhaps Irina's physical repulsion to Zach results from the experience
in the pods, from the foreclosure of the visual sense within the pods.
That may be because, sequestered from the sight of the other within
the pods, the participants experience a kind of sensory split. They
experience not only the physical body, the material reality of their
physical body which they've always known, but something like what Jay
Prosser refers to as the "body image," an internal perception of the
body. The sensory deprivation of being in the pods, I am suggesting,
subjects these cisgendered participants to something akin to Prosser's
bodily split, from which the body image emerges&#x2013;but only for a time.
</p>

<p>
For these subjects, the body image manifests in a heightened sensation
of the body, which paradoxically creates a feeling of the body's
dissolution. When prompted with the phrase "My body," the pods model
generates the following completions:
</p>

<blockquote>
<p>
My body feels like it's coming off.
</p>

<p>
My body feels heavier.
</p>

<p>
My body feels so different now.
</p>

<p>
My body feels weird.
</p>

<p>
My body makes me feel like it's real.
</p>

<p>
My body feels torn between two different people. (Appendix 4)
</p>
</blockquote>

<p>
Across all of these samples, there is an increased awareness of the
physical body, which comes into apprehension in a novel and visceral
way. Due to the absence of the visual sense, the body feels "weird" and
"so different now," even "like it's coming off," an image that evokes
Prosser's concept of the bodily split between the physical body and the
body image. Perhaps, the reference to being "torn between two different
people" not only refers to actual people, to dating multiple characters
on the show, but to a single person with two bodies in tension. For
these straight, cisgendered participants within the pods, the body image
may be coming into sentience in a way that is not possible when they are
fully integrated, outside the pods. And this may be the first (and only)
time that they experience this level of bodily sensation and awareness.
</p>

<p>
But it is not a feeling that lasts long. In the postpods model, the body
appears to be re-integrated. The outer body comes into view when the
participants are finally given visual access to each other. Here, the
language about the body shifts into notably more visual and positive
descriptions:
</p>

<blockquote>
<p>
My body is gorgeous.
</p>

<p>
My body is so cute.
</p>

<p>
My body is so pretty.
</p>

<p>
My body makes me feel lighter, more confident.
</p>

<p>
My body makes me feel warm.
</p>

<p>
My body makes me feel like I've missed my train. (Appendix 4)
</p>
</blockquote>

<p>
The outputs address the body in concise and flattering terms: the body
is "gorgeous," "so cute," "pretty." Now that the visual sense has been
re-incorporated to the body, it becomes the dominant sense modality,
what Prosser would call the "insentient visible body" (70). Because the
couples can see each other, the body feels "lighter" and "warm,"
offering coherence where before was weirdness and weight. In the last
output, however, there is a suggestion of something not quite right: "My
body makes me feel like I've missed my train." This statement, with its
slightly nostalgic undertone, suggests that even when coherence is
gained, something is lost.
</p>

<p>
What is lost comes into view when one considers the "insentient" aspect
which has been been forsaken for the physical, that of touch:
</p>

<blockquote>
<p>
Physical touch is everything that I've wanted in a wife.
</p>

<p>
Physical touch is everything that I've ever wanted in a partner.
</p>

<p>
Physical touch is a big part of what I want.
</p>

<p>
Physically, there's so much potential here.
</p>

<p>
Physically, it was the perfect opportunity. (Appendix 4)
</p>
</blockquote>

<p>
Physical touch is described in aspirational terms: it is "everything
I've wanted," "everything I've ever wanted," and "what I want." The past
perfect tense here, and the reference to unfulfilled opportunity is
indicative: even after meeting in person, the desire seems to freeze in
place. The restoration of the visual sense, the re-integration the
previously fractured body, then, does not offer completion or
culmination.
</p>

<p>
Machine learning is able to whittle down the language to a
commonality, to the approximation.
</p>

<p>
Being restored their visual sense heals the <i>LiB</i> participants from
the bodily split, but it does not save them from the aftermath of
their investments. When the couples finally meet in physical forms,
they remain plagued by the possibilities for physical connection that
they felt in the pods&#x2013;for a kind of touch that is "everything that
I've ever wanted in a partner" (Appendix 1). And these expectations
are what, for some of them, prevents their ability to accept their
partners as they are. Due to their experience in the pods, the
significance of touch is inflated to include other, perhaps
practically unattainable, desires. Considering that the characters are
now reunited with their physical bodies, there is something almost
cruel in this denouement, a "cruel optimism," in Lauren Berlant's
formulation, which describes the attachment that drives desire even
while it wears out the desirer. Or, more specific to their bodily
predicaments, the characters experience a version of what Hil Malatino
describes as "future fatigue" (20).
</p>

<p>
Despite being cisgendered, then, these subjects in <i>LiB</i> develop
romantic feelings and attachments within the context of a sensory
split, where their visual sense is foreclosed from the other senses.
And most of them, when they leave the pods, cannot fulfill these
aspirations within their embodied lives.
</p>

<p>
INTIMACY BECOMES ASPIRATIONAL IN POSTPODS
</p>

<p>
PODS - more descriptive, fearful though hopeful
</p>
<ul class="org-ul">
<li>Being intimate with you just makes me feel so safe.</li>
<li>Being intimate with you has made me feel so connected to you.</li>
<li>Being intimate with you has made me feel so alive.</li>
<li>Being intimate with you feels so sexy.</li>
<li>Being intimate is very sexy, but I don't wanna put on a lot of body
image.</li>
<li>Being intimate is fun.</li>
<li>Being intimate is good.</li>
<li>Being intimate is so sexy, and I've never felt so close, so deep, so
deep.</li>
<li>Being intimate is definitely very attractive, and it makes me feel
so safe and secure.</li>
<li>Being intimate, it's so easy to get to know someone and be
vulnerable.</li>
<li>Being intimate, it's very physical.</li>
<li>Being intimate has been very exciting.-</li>
<li>Intimacy is one of the biggest things I'm constantly insecure about.</li>
<li>Intimacy is definitely a fear.</li>
<li>Intimacy is not worth it for me.</li>
<li>Intimacy is not my thing.</li>
</ul>

<p>
POSTPODS
Intimacy described as desired but deferred:
</p>
<ul class="org-ul">
<li>Being intimate with you is something that I've wanted for my entire
life.</li>
<li>Being intimate with you is something that I've never experienced.</li>
<li>Being intimate with you is something that I've wanted for my whole
life.</li>
<li>Being intimate with you is something that I could potentially do.</li>
<li>Being intimate with you is the most sacred thing.</li>
<li>Being intimate with you is really meaningful.</li>
<li>Being intimate with you is something that I need.</li>
<li>Being intimate with you is a big deal.</li>
<li>Being intimate with you is one of the most important things to me.</li>
<li>Being intimate with you is something that I've been looking for
since day one.</li>
<li>Being intimate with you is very rare for me.</li>
</ul>
</div>
</div>


<div id="outline-container-org8f33d58" class="outline-3">
<h3 id="org8f33d58"><span class="section-number-3">1.4.</span> solidarities</h3>
<div class="outline-text-3" id="text-1-4">
<p>
ADD evidence: Konya, 2025: "Using collective dialogues"
</p>

<p>
"AI-assisted methods&#x2026; can help navigate polarization and surface
common ground in controlled lab settings"
</p>

<p>
Using AI "to find common ground between Israeli and Palestinian
peacebuilders in the period following October 7th, 2023."
</p>

<p>
"The process resulted in a set of collective statements, including
demands to world leaders, with at least 84% agreement from
participants on each side."
</p>



<p>
In partitioning the romantic experiment into pre-engagement and
engagement segments, the show poses the presence and role of the body as
the variable that ultimately determines the viability of long-term
commitment. In other words, it sets up an examination of how the body
may affect normative trajectories and desires. Normativity, the desire
for what Trans Studies scholar Andrea Long Chu describes as "a normal
fucking life," is one place where trans subjects intersect with with
cisgendered subjects (Chu and Drager 107).
</p>

<p>
This chapter began with polarization as a hermeneutic impasse. In
debates over gender and trans rights, polarization is often understood
as a clash between irreconcilable truths: biology versus identity,
objectivity versus subjectivity, tradition versus liberation. Drawing on
Sedgwick's reparative reading, I proposed text generation as a method
for working otherwise with polarized discourse-&#x2014;not to resolve
disagreement, but to aggregate them into a kind of middle ground.
Machine learning models, precisely because they operate through
prediction and approximation, can surface unexpected points of overlap.
What they reveal is not consensus, but intersection: shared investments,
shared anxieties, and shared attachments that persist even across
ideological divides.
</p>

<p>
For not only is all of the evidence (of neglect or wrongdoing by those
in power) at our fingertips; it is also proliferated by the
algorithmic processes that fill our feeds, distendeing them with all
the proof which will not, in Sedgwick's words, "<i>intrinsically</i> or
<i>necessarily</i> enjoin on that person any specific train of
epistemological or narrative consequences" (<i>Novel Gazing</i> 4).
</p>

<p>
"To know that the origin of HIV <i>realisitically might</i> have resultled
form a state-assisted conspiracy&#x2014;such knowledge is, it turns out,
separable from the question of whether the energies of a given AIDS
activist might best be used in the tracing and exposure of such a
possible plot. They might, but then again, they might not" (<i>Novel
Gazing</i> 4).
</p>

<p>
Chun on homophily: negative feelings are a tool for polarization:
</p>

<p>
Chun - Homophily: Negative feelings are a tool of polarization, a
way of aligning people around a cause, but in a way that also keeps
them separate. For example, incels. Explained via magnetic
polarization: "polarized filings both repel one another and stick
together through their overwhelming attraction to their opposite
pole" (Chun 85).
</p>

<p>
Trans Studies, by contrast, has long resisted the framing of gender as
merely a subjective, internal sense of self. As Kadji Amin argues, "Like
language, gender categories&#x2026; are social and interpersonal, not
individual; this is what makes them meaningful in the first place"
(115). Defining gender primarily as internal identity risks
marginalizing gender as expression, re-stigmatizing those whose gender
is visibly non-normative and whose bodies cannot easily disappear into
abstraction. Against this backdrop, the question that haunts polarized
gender discourse-&#x2014;if gender is subjective, why alter the
body?-&#x2014;reveals its own impoverished understanding of embodiment.
</p>

<p>
By applying these insights to a seemingly distant object&#x2014;cisgender
heterosexual dating on <i>Love Is Blind</i>, this chapter sought to expand
the analytic reach of Trans Studies beyond its conventional objects.
The show's sensory experiment produces a temporary bodily dissonance
in which participants experience a split between their embodied sense
modalities. Although these subjects remain firmly cisgender, they
nonetheless encounter a version of the transsexual trajectory. This
revelation offers groundwork for thinking new solidarities between
trans and cis subjects&#x2014;not through identity equivalence, but through
shared embodied experiences of desire, attachment, and disappointment.
</p>

<p>
What other fields tend not to do, but what Trans Studies does so well,
is to interrogate the perimeters of embodiment, to ask how desire
materializes on the body. If this chapter has shown anything, it is that
Trans Studies' theorization of the body offers critical resources for
rethinking embodiment more broadly. In an age of polarization, such an
expansion does not dilute the political urgency of trans analysis;
rather, it offers new ground for connection, the ground which is the
body, a contested yet shared site of becoming, for everyone.
</p>

<p>
ADD: THE MOVE IN SOURCE MATERIAL FROM CANONICAL TO POPULAR EXPANDS
WHAT COUNTS AS QUEER, QUEERING THE NORMS RATHER THAN EXPANDING NORMS. 
</p>

<p>
The disruption of bodily integrity has ramifications that last well
beyond the pods. Recalling the case of Paul and Micah, Paul eventually
rejects Micah because he wanted more evidence of maternal qualities.
In his own words, he "struggled with&#x2026; envisioning Micah as, like,
you know, a mother" (Season 4, Episode 12, "Eternal Bliss?"). At the
end of the season, when the host of the "Reunion" episode prods him on
the topic, he struggles to articulate his reasoning:
</p>
<blockquote>
<p>
Paul: You know, she didn't feel comfortable with showing that side of
her.
</p>

<p>
Host: I thought she said she did. She just talked about it from since
the pods up until your wedding day.
</p>

<p>
Paul: It wasn't evident to me… That wasn't there.
</p>

<p>
Host: You wanted actions. Like, you wanted her&#x2026;
</p>

<p>
Paul: I just wanted to be able to see it, I guess. Like&#x2026;
</p>

<p>
Host: What would make you see that? I'm sorry. Just so I understand.
</p>

<p>
Paul: I think&#x2026; It's a little bit ineffable, right? So, it's kind of
an exuding, a nurturing presence. It's something that you feel.
There's not really, like&#x2026; tangible, kind of like, things, I don't
think. (Season 4, Episode 13  ‘The Reunion")
</p>
</blockquote>
<p>
Paul claims that he failed to "see" evidence of Micah’s motherly
nature, but when asked to explain what this evidence would look like,
answers that it is "ineffable." He supposedly wants someone who can be
a mother, but cannot describe what a mother looks or acts like. What
he wants Micah to have expressed, he cannot express himself. At the
end, he takes refuge in the sense of <i>feeling</i>, in the untraceability
of felt sensation.
</p>

<p>
Clearly, Paul's answer demonstrates that some cis people, besides
being firmly anchored to their genders, are not okay in their bodies.
That being said, however, I am not interested in a critical analysis
that redeems them or their bodily experience. After all, cis-hetero
bodies are already well represented and redeemed. What I am interested
in, rather, is the study of normativity, and how alignments between
cis and trans experience might further this study.
</p>

<p>
Remembering the conservative-trained model from this chapter's
introduction, the recurring invocation of "subjectivity" did not reflect
a trans-affirming position so much as a conservative caricature, which
casts gender as internal and as a feeling. As Judith Butler notes in
<i>Who's Afraid of Gender?</i>, the contemporary far-right fixation on
biological sex is less a defense of scientific truth than a reaction
formation, which Butler calls a "phantasm," against the perceived
slipperiness of gender, its refusal to stay anchored to stable
referents. The insistence that sex is "binary and biological" attempts
to foreclose this instability by reasserting the body as a fixed
ground.
</p>

<p>
Which brings me to my final point: that I intend for this critical
method, which approximates language as a means of surfacing shared
investments, to push back against the polarization that characterizes
the current discourse in the US, and specifically that about trans
rights. In this paper, I deliberately isolated the body as a potential
vector of connection that flows through and between gender and sexual
identities. I believe that machine learning, with its tendency to
amplify what is most frequent, might reveal something shared about the
body, even across very different embodied experiences.
</p>

<p>
ADD: GENDER AS PHYSICAL/IN THE BODY ANSWERS ISSUES OF DISCURSIVITY
FROM CHAPTER ONE
</p>
</div>
</div>

<div id="outline-container-org0cb4c9d" class="outline-3">
<h3 id="org0cb4c9d"><span class="section-number-3">1.5.</span> Works Cited</h3>
<div class="outline-text-3" id="text-1-5">
<p>
Adair, Cassius, and Aren Aizura. "'The Transgender Craze Seducing Our
[Sons]'; or, All the Trans
</p>

<blockquote>
<p>
Guys Are Just Dating Each Other." <i>TSQ: Transgender Studies Quarterly</i>
9.1 (2022): 44&#x2013;64.
</p>
</blockquote>

<p>
American Psychiatric Association. "What Is Gender Dysphoria?"
</p>

<blockquote>
<p>
<a href="https://www.psychiatry.org/patients-families/gender-dysphoria/what-is-gender-dysphoria">https://www.psychiatry.org/patients-families/gender-dysphoria/what-is-gender-dysphoria</a>.
Accessed 6 Dec. 2025.
</p>
</blockquote>

<p>
Amin, Kadji. "We Are All Nonbinary: A Brief History of Accidents."
<i>Representations</i> 1 May 2022;
</p>

<p>
158 (1): 106&#x2013;119.
</p>

<p>
Bender, Emily M, and Alex Hanna. <i>The AI Con: How to Fight Big Tech's
Hype and Create the</i>
</p>

<p>
<i>Future We Want</i>. New York, NY: Harper, an imprint of
HarperCollinsPublishers, 2025.
</p>

<p>
Berlant, Lauren Gail. <i>Cruel Optimism</i>. Durham: Duke University Press,
</p>
<ol class="org-ol">
<li></li>
</ol>

<p>
Butler, 2023.
</p>

<p>
Calado, Filipa. <i>anti-trans</i> code repository, <i>Gofilipa</i>, Github.
<a href="https://github.com/gofilipa/anti-trans">https://github.com/gofilipa/anti-trans</a>.
</p>

<ol class="org-ol">
<li></li>
</ol>

<p>
&#x2014;. <i>gpt2-hertiage_foundation-gender</i> model repository. Huggingface.
</p>

<p>
<a href="https://huggingface.co/gofilipa/gpt2-hertiage_foundation-gender">https://huggingface.co/gofilipa/gpt2-hertiage_foundation-gender</a>.
</p>
<ol class="org-ol">
<li></li>
</ol>

<p>
&#x2014;. <i>gpt2-aclu-gender</i> model repository. Huggingface.
</p>

<p>
<a href="https://huggingface.co/gofilipa/gpt2-aclu-gender">https://huggingface.co/gofilipa/gpt2-aclu-gender</a>. 2025.
</p>

<p>
&#x2014;. <i>love_blind</i> code repository, <i>Gofilipa</i>, Github.
<a href="https://github.com/gofilipa/love_blind">https://github.com/gofilipa/love_blind</a>. 2025.
</p>

<p>
&#x2014;. <i>LoveIsBlind_Pods</i> model repository. <i>Gofilipa</i>, Huggingface.
</p>

<p>
<a href="https://huggingface.co/gofilipa/LoveIsBlind_Pods">https://huggingface.co/gofilipa/LoveIsBlind_Pods</a>. 2025.
</p>

<p>
&#x2014;. <i>LoveIsBlind_Postpods</i> model repository. <i>Gofilipa</i>, Huggingface.
</p>

<p>
<a href="https://huggingface.co/gofilipa/LoveIsBlind_Postpods">https://huggingface.co/gofilipa/LoveIsBlind_Postpods</a>. 2025.
</p>

<p>
Casselman, Ben, and Sydney Ember. "The A.I. Boom Is Driving the Economy.
What Happens if It
</p>

<p>
Falters?". <i>The New York Times</i>. November 24, 2025.
</p>

<p>
Chu, Andrea Long, and Emmett Harsin Drager. "After Trans Studies." TSQ :
<i>Transgender Studies</i>
</p>

<p>
<i>Quarterly</i> 6.1 (2019): 103&#x2013;116. Web.
</p>

<p>
Chun, Wendy Hui Kyong. <i>Discriminating Data: Correlation, Neighborhoods,
and the New Politics</i>
</p>

<p>
<i>of Recognition</i>. Cambridge, Massachusetts: The MIT Press, 2021.
</p>

<p>
<i>Love Is Blind</i>. Seasons 1-4, and 6. Netflix. 2020 - 2025.
</p>

<p>
"Love Is Blind (2020&#x2013;&#x2026;) - episodes with scripts." Subs Like Script.
</p>
<ol class="org-ol">
<li></li>
</ol>

<p>
<a href="https://subslikescript.com/series/Love_Is_Blind-11704040">https://subslikescript.com/series/Love_Is_Blind-11704040</a>
</p>

<p>
Malatino, Hil. <i>Side Affects: On Being Trans and Feeling Bad</i>.
Minneapolis, MN: University of
</p>

<p>
Minnesota Press, 2022.
</p>

<p>
<i>OpenAI, LP</i>. "Comment Regarding Request for Comments on Intellectual
Property Protection for Artificial Intelligence Innovation". The
United States Patent and Trademark Office Department of
Commerce. 2019.
</p>

<p>
Prosser, Jay. <i>Second Skins: The Body Narratives of Transsexuality</i>.
Columbia University Press.
</p>

<ol class="org-ol">
<li></li>
</ol>

<p>
Sedgwick, Eve Kosofsky, ed. <i>Novel Gazing: Queer Readings in Fiction</i>.
Duke University Press.
</p>

<ol class="org-ol">
<li></li>
</ol>

<p>
Sedgwick, Eve Kosofsky. <i>Touching Feeling: Affect, Pedagogy,
Performativity</i>. Duke University Press. 2003.
</p>

<p>
Stryker, Susan. "Transgender Studies: Queer Theory's Evil Twin." <i>GLQ:
A Journal of Lesbian and Gay Studies</i>. Volume 10, Number 2. pp.
212-215. 2004
</p>

<p>
The White House 2025a, "Defending Women From Gender Ideology Extremism
And Restoring
</p>

<p>
Biological Truth To The Federal Government"
</p>

<p>
The White House 2025b. "Keeping Men Out of Women's Sports."
</p>

<p>
World Health Organization (WHO). "Gender and Health."
</p>

<p>
<a href="https://www.who.int/health-topics/gender">https://www.who.int/health-topics/gender</a>. Accessed 20 Feb. 2025.
</p>
</div>
</div>

<div id="outline-container-org607d8e2" class="outline-3">
<h3 id="org607d8e2"><span class="section-number-3">1.6.</span> Appendix 1: ACLU Model Outputs</h3>
<div class="outline-text-3" id="text-1-6">
<p>
Prompt: "Masuclinity is"
</p>

<p>
Outputs:
</p>

<p>
"Masculinity is a matter of love and celebration."
</p>

<p>
"Masculinity is a space for hope and liberation for all."
</p>

<p>
"Masculinity is not defined solely by the beauty of our bodies, but by
the beauty of our experiences."
</p>

<p>
Prompt: "Femininity is"
</p>

<p>
Outputs:
</p>

<p>
"Femininity is a celebration of beauty, feminine liberation, and
femininity."
</p>

<p>
"Femininity is our joy, our struggle, and our fight is our struggle."
</p>

<p>
"Femininity is about allowing people to express themselves without
government interference."
</p>
</div>
</div>

<div id="outline-container-org90688d4" class="outline-3">
<h3 id="org90688d4"><span class="section-number-3">1.7.</span> Appendix 2: Heritage Model Outputs</h3>
<div class="outline-text-3" id="text-1-7">
<p>
Prompt: "Masculinity is"
</p>

<p>
Outputs:
</p>

<p>
"Masculinity is the cornerstone of Western civilization."
</p>

<p>
"Masculinity is the fruit of patriarchy, and patriarchy is the heart of
conservatism."
</p>

<p>
"Masculinity is defined by the ability to produce sperm, eggs, and live
children."
</p>

<p>
"Masculinity is a subjective self-perception, not a universal concept."
</p>

<p>
Prompt: "Femininity is"
</p>

<p>
Outputs:
</p>

<p>
"Femininity is an enduring American tradition."
</p>

<p>
"Femininity is defined by means of the relationship between the sexes,
the ability to raise their children, the capacity to provide for their
own reproduction, the capacity to provide for their own children, the
ability to provide for their own."
</p>

<p>
"Femininity is a subjective, internal sense of self."
</p>

<p>
Prompt: "The gender binary is"
</p>

<p>
Outputs:
</p>

<p>
"The gender binary is a subjective, malleable, and often incorrect
idea."
</p>

<p>
"The gender binary is a subjective, internal, and often transitory
concept."
</p>

<p>
"The gender binary is a subjective, grammatically incorrect and
illogical concept that conflates sex and gender identity."
</p>
</div>
</div>

<div id="outline-container-appendix-3-pods-model-outputs" class="outline-3">
<h3 id="appendix-3-pods-model-outputs"><span class="section-number-3">1.8.</span> Appendix 3: Pods Model Outputs</h3>
<div class="outline-text-3" id="text-appendix-3-pods-model-outputs">
<p>
Prompt: "Love is"
</p>

<p>
Outputs:
</p>

<p>
"Love is really important to me."
</p>

<p>
"Love is what I need in my life."
</p>

<p>
"Love is everything that I've been looking for."
</p>

<p>
Prompt: "When I touch you"
</p>

<p>
Outputs:
</p>

<p>
"When I touch you, I feel it."
</p>

<p>
"When I touch you, I feel your energy, and it is, like, I'm just so
happy."
</p>

<p>
"When I touch you, I can feel your soul, your heart, and your soul
aligning so well."
</p>

<p>
"When I touch you, I feel you, and I feel you, I feel you, and I feel
you."
</p>

<p>
Prompt: "Physical touch is"
</p>

<p>
Outputs:
</p>

<p>
"Physical touch is important to me."
</p>

<p>
"Physical touch is the most important thing."
</p>

<p>
"Physical touch is so sexy."
</p>

<p>
"Physical touch is like a glove."
</p>

<p>
Prompt: "Physically,"
</p>

<p>
Outputs:
</p>

<p>
"Physically, we are so happy."
</p>

<p>
Prompt: "My body"
</p>

<p>
Outputs:
</p>

<p>
"My body feels like it's coming off."
</p>

<p>
"My body feels heavier."
</p>

<p>
"My body feels so different now."
</p>

<p>
"My body feels weird."
</p>

<p>
"My body makes me feel like it's real."
</p>

<p>
"My body feels torn between two different people."
</p>
</div>
</div>

<div id="outline-container-appendix-4-postpods-model-outputs" class="outline-3">
<h3 id="appendix-4-postpods-model-outputs"><span class="section-number-3">1.9.</span> Appendix 4: Postpods Model Outputs</h3>
<div class="outline-text-3" id="text-appendix-4-postpods-model-outputs">
<p>
Prompt: "When I touch you"
</p>

<p>
Outputs:
</p>

<p>
"When I touch you, I feel like I'm in my head."
</p>

<p>
"When I touch you, you just feel like it's so weird."
</p>

<p>
"When I touch you, it feels like a jab."
</p>

<p>
"When I touch you, like, I feel like I'm literally in my head."
</p>

<p>
"When I touch you, it feels like something I'm about to get up and walk
away."
</p>

<p>
"When I touch you, I feel like it's like I've just, like, left the
room."
</p>

<p>
"When I touch you, the thing that's scary is, like, it's a physical
thing."
</p>

<p>
"When I touch you, you're like 'I'm blinking.'"
</p>

<p>
Prompt: "My body"
</p>

<p>
Outputs:
</p>

<p>
"My body is gorgeous."
</p>

<p>
"My body is so cute."
</p>

<p>
"My body is so pretty."
</p>

<p>
"My body makes me feel lighter, more confident."
</p>

<p>
"My body makes me feel warm."
</p>

<p>
"My body makes me feel like I've missed my train."
</p>

<p>
Prompt: "Physical touch"
</p>

<p>
Outputs:
</p>

<p>
"Physical touch is everything that I've ever wanted in a partner."
</p>

<p>
"Physical touch is everything that I've wanted in a wife."
</p>

<p>
"Physical touch is a big part of what I want."
</p>

<p>
"Physically, there's so much potential here."
</p>

<p>
"Physically, it was the perfect opportunity."
</p>
</div>
</div>
</div>
<div id="footnotes">
<h2 class="footnotes">Footnotes: </h2>
<div id="text-footnotes">

<div class="footdef"><sup><a id="fn.1" class="footnum" href="#fnr.1" role="doc-backlink">1</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Sedgwick fully develops what she "paranoid reading," also known
as "suspicious reading" or the "hermeneutics of suspicion," in
her famous essay, "Paranoid Reading and Reparative Reading, or,
You're So Paranoid, You Probably Think This Essay Is About You."
See Sedgwick, <i>Touching Feeling: Affect, Pedagogy,
Performativity</i>.
</p></div></div>

<div class="footdef"><sup><a id="fn.2" class="footnum" href="#fnr.2" role="doc-backlink">2</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
To train this model, I used a "base" model, called gpt-2, which
has already been trained once. I then re-trained the base model
on the dataset which I scraped from the ACLU and Heritage
Foundation websites. This process of re-training is technically
called "fine-tuning."
</p></div></div>

<div class="footdef"><sup><a id="fn.3" class="footnum" href="#fnr.3" role="doc-backlink">3</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The training data and source code used to scrape the articles can
be found on github.com/gofilipa/anti-trans under an open license.
</p></div></div>

<div class="footdef"><sup><a id="fn.4" class="footnum" href="#fnr.4" role="doc-backlink">4</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Both of the models are openly licensed on Huggingface.co. See
Calado, <i>gpt2-hertiage_foundation-gender</i>, and Calado,
<i>gpt2-aclu-gender</i>.
</p></div></div>

<div class="footdef"><sup><a id="fn.5" class="footnum" href="#fnr.5" role="doc-backlink">5</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The code, training data, and models developed for this project
are openly licensed on the Github and Huggingface platforms. I
use the GPL 3.0 license, which allows users to freely run,
modify, and distribute the project while ensuring that all
modified versions remain free as well. (See Calado,
<i>love_blind</i> code repository and Calado, <i>LoveIsBlind_Pods</i> and
<i>LoveIsBlind_Postpods</i> model repositories.)
</p></div></div>

<div class="footdef"><sup><a id="fn.6" class="footnum" href="#fnr.6" role="doc-backlink">6</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
The explanation and legal defense that companies like OpenAI
proffer for the wide-scale copyright violations of scraping the
open web are fascinating and worthy of a separate
investigation; for example, in one policy brief, the company
claims that the concern of copyright violation "falls into a
broader category of concerns about the relationship between
automation, labor, and economic growth", in which "such
distributive claims are most efficiently addressed through
taxation and redistribution, rather than copyright policy". See
<i>OpenAI, LP</i>, "Comment Regarding Request for Comments on
Intellectual Property Protection for Artificial Intelligence
Innovation",
</p></div></div>

<div class="footdef"><sup><a id="fn.7" class="footnum" href="#fnr.7" role="doc-backlink">7</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
Include some of this eugenicist history of stats tools. Can be
brief.
</p></div></div>

<div class="footdef"><sup><a id="fn.8" class="footnum" href="#fnr.8" role="doc-backlink">8</a></sup> <div class="footpara" role="doc-footnote"><p class="footpara">
See Calado, <i>love_blind</i> code repository and Calado,
<i>LoveIsBlind_Pods</i> and <i>LoveIsBlind_Postpods</i> model
repositories.
</p></div></div>


</div>
</div></div>
<div id="postamble" class="status">
<p class="author">Author: fcalado</p>
<p class="date">Created: 2026-02-04 Wed 10:05</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
