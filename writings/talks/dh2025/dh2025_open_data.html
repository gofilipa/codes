<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN"
"http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
<!-- 2025-07-11 Fri 17:58 -->
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<meta name="viewport" content="width=device-width, initial-scale=1" />
<title>&lrm;</title>
<meta name="author" content="fcalado" />
<meta name="generator" content="Org Mode" />
<style>
  #content { max-width: 60em; margin: auto; }
  .title  { text-align: center;
             margin-bottom: .2em; }
  .subtitle { text-align: center;
              font-size: medium;
              font-weight: bold;
              margin-top:0; }
  .todo   { font-family: monospace; color: red; }
  .done   { font-family: monospace; color: green; }
  .priority { font-family: monospace; color: orange; }
  .tag    { background-color: #eee; font-family: monospace;
            padding: 2px; font-size: 80%; font-weight: normal; }
  .timestamp { color: #bebebe; }
  .timestamp-kwd { color: #5f9ea0; }
  .org-right  { margin-left: auto; margin-right: 0px;  text-align: right; }
  .org-left   { margin-left: 0px;  margin-right: auto; text-align: left; }
  .org-center { margin-left: auto; margin-right: auto; text-align: center; }
  .underline { text-decoration: underline; }
  #postamble p, #preamble p { font-size: 90%; margin: .2em; }
  p.verse { margin-left: 3%; }
  pre {
    border: 1px solid #e6e6e6;
    border-radius: 3px;
    background-color: #f2f2f2;
    padding: 8pt;
    font-family: monospace;
    overflow: auto;
    margin: 1.2em;
  }
  pre.src {
    position: relative;
    overflow: auto;
  }
  pre.src:before {
    display: none;
    position: absolute;
    top: -8px;
    right: 12px;
    padding: 3px;
    color: #555;
    background-color: #f2f2f299;
  }
  pre.src:hover:before { display: inline; margin-top: 14px;}
  /* Languages per Org manual */
  pre.src-asymptote:before { content: 'Asymptote'; }
  pre.src-awk:before { content: 'Awk'; }
  pre.src-authinfo::before { content: 'Authinfo'; }
  pre.src-C:before { content: 'C'; }
  /* pre.src-C++ doesn't work in CSS */
  pre.src-clojure:before { content: 'Clojure'; }
  pre.src-css:before { content: 'CSS'; }
  pre.src-D:before { content: 'D'; }
  pre.src-ditaa:before { content: 'ditaa'; }
  pre.src-dot:before { content: 'Graphviz'; }
  pre.src-calc:before { content: 'Emacs Calc'; }
  pre.src-emacs-lisp:before { content: 'Emacs Lisp'; }
  pre.src-fortran:before { content: 'Fortran'; }
  pre.src-gnuplot:before { content: 'gnuplot'; }
  pre.src-haskell:before { content: 'Haskell'; }
  pre.src-hledger:before { content: 'hledger'; }
  pre.src-java:before { content: 'Java'; }
  pre.src-js:before { content: 'Javascript'; }
  pre.src-latex:before { content: 'LaTeX'; }
  pre.src-ledger:before { content: 'Ledger'; }
  pre.src-lisp:before { content: 'Lisp'; }
  pre.src-lilypond:before { content: 'Lilypond'; }
  pre.src-lua:before { content: 'Lua'; }
  pre.src-matlab:before { content: 'MATLAB'; }
  pre.src-mscgen:before { content: 'Mscgen'; }
  pre.src-ocaml:before { content: 'Objective Caml'; }
  pre.src-octave:before { content: 'Octave'; }
  pre.src-org:before { content: 'Org mode'; }
  pre.src-oz:before { content: 'OZ'; }
  pre.src-plantuml:before { content: 'Plantuml'; }
  pre.src-processing:before { content: 'Processing.js'; }
  pre.src-python:before { content: 'Python'; }
  pre.src-R:before { content: 'R'; }
  pre.src-ruby:before { content: 'Ruby'; }
  pre.src-sass:before { content: 'Sass'; }
  pre.src-scheme:before { content: 'Scheme'; }
  pre.src-screen:before { content: 'Gnu Screen'; }
  pre.src-sed:before { content: 'Sed'; }
  pre.src-sh:before { content: 'shell'; }
  pre.src-sql:before { content: 'SQL'; }
  pre.src-sqlite:before { content: 'SQLite'; }
  /* additional languages in org.el's org-babel-load-languages alist */
  pre.src-forth:before { content: 'Forth'; }
  pre.src-io:before { content: 'IO'; }
  pre.src-J:before { content: 'J'; }
  pre.src-makefile:before { content: 'Makefile'; }
  pre.src-maxima:before { content: 'Maxima'; }
  pre.src-perl:before { content: 'Perl'; }
  pre.src-picolisp:before { content: 'Pico Lisp'; }
  pre.src-scala:before { content: 'Scala'; }
  pre.src-shell:before { content: 'Shell Script'; }
  pre.src-ebnf2ps:before { content: 'ebfn2ps'; }
  /* additional language identifiers per "defun org-babel-execute"
       in ob-*.el */
  pre.src-cpp:before  { content: 'C++'; }
  pre.src-abc:before  { content: 'ABC'; }
  pre.src-coq:before  { content: 'Coq'; }
  pre.src-groovy:before  { content: 'Groovy'; }
  /* additional language identifiers from org-babel-shell-names in
     ob-shell.el: ob-shell is the only babel language using a lambda to put
     the execution function name together. */
  pre.src-bash:before  { content: 'bash'; }
  pre.src-csh:before  { content: 'csh'; }
  pre.src-ash:before  { content: 'ash'; }
  pre.src-dash:before  { content: 'dash'; }
  pre.src-ksh:before  { content: 'ksh'; }
  pre.src-mksh:before  { content: 'mksh'; }
  pre.src-posh:before  { content: 'posh'; }
  /* Additional Emacs modes also supported by the LaTeX listings package */
  pre.src-ada:before { content: 'Ada'; }
  pre.src-asm:before { content: 'Assembler'; }
  pre.src-caml:before { content: 'Caml'; }
  pre.src-delphi:before { content: 'Delphi'; }
  pre.src-html:before { content: 'HTML'; }
  pre.src-idl:before { content: 'IDL'; }
  pre.src-mercury:before { content: 'Mercury'; }
  pre.src-metapost:before { content: 'MetaPost'; }
  pre.src-modula-2:before { content: 'Modula-2'; }
  pre.src-pascal:before { content: 'Pascal'; }
  pre.src-ps:before { content: 'PostScript'; }
  pre.src-prolog:before { content: 'Prolog'; }
  pre.src-simula:before { content: 'Simula'; }
  pre.src-tcl:before { content: 'tcl'; }
  pre.src-tex:before { content: 'TeX'; }
  pre.src-plain-tex:before { content: 'Plain TeX'; }
  pre.src-verilog:before { content: 'Verilog'; }
  pre.src-vhdl:before { content: 'VHDL'; }
  pre.src-xml:before { content: 'XML'; }
  pre.src-nxml:before { content: 'XML'; }
  /* add a generic configuration mode; LaTeX export needs an additional
     (add-to-list 'org-latex-listings-langs '(conf " ")) in .emacs */
  pre.src-conf:before { content: 'Configuration File'; }

  table { border-collapse:collapse; }
  caption.t-above { caption-side: top; }
  caption.t-bottom { caption-side: bottom; }
  td, th { vertical-align:top;  }
  th.org-right  { text-align: center;  }
  th.org-left   { text-align: center;   }
  th.org-center { text-align: center; }
  td.org-right  { text-align: right;  }
  td.org-left   { text-align: left;   }
  td.org-center { text-align: center; }
  dt { font-weight: bold; }
  .footpara { display: inline; }
  .footdef  { margin-bottom: 1em; }
  .figure { padding: 1em; }
  .figure p { text-align: center; }
  .equation-container {
    display: table;
    text-align: center;
    width: 100%;
  }
  .equation {
    vertical-align: middle;
  }
  .equation-label {
    display: table-cell;
    text-align: right;
    vertical-align: middle;
  }
  .inlinetask {
    padding: 10px;
    border: 2px solid gray;
    margin: 10px;
    background: #ffffcc;
  }
  #org-div-home-and-up
   { text-align: right; font-size: 70%; white-space: nowrap; }
  textarea { overflow-x: auto; }
  .linenr { font-size: smaller }
  .code-highlighted { background-color: #ffff00; }
  .org-info-js_info-navigation { border-style: none; }
  #org-info-js_console-label
    { font-size: 10px; font-weight: bold; white-space: nowrap; }
  .org-info-js_search-highlight
    { background-color: #ffff00; color: #000000; font-weight: bold; }
  .org-svg { }
</style>
</head>
<body>
<div id="content" class="content">
<div id="table-of-contents" role="doc-toc">
<h2>Table of Contents</h2>
<div id="text-table-of-contents" role="doc-toc">
<ul>
<li><a href="#org45692e1">1. draft</a>
<ul>
<li><a href="#org4ba0105">1.1. introduction</a></li>
<li><a href="#orgbdc67a9">1.2. the defense of "transformative"</a></li>
<li><a href="#orgdf85af2">1.3. close reading</a></li>
</ul>
</li>
</ul>
</div>
</div>
<div id="outline-container-org45692e1" class="outline-2">
<h2 id="org45692e1"><span class="section-number-2">1.</span> draft</h2>
<div class="outline-text-2" id="text-1">
</div>
<div id="outline-container-org4ba0105" class="outline-3">
<h3 id="org4ba0105"><span class="section-number-3">1.1.</span> introduction</h3>
<div class="outline-text-3" id="text-1-1">
<p>
I'm going to speak on this topic of openness and open data from my
perspective of a programmer, as a person who actively uses ML tools in
her research and her teaching.
</p>

<p>
While these tools are deeply problematic, and using them immediately
implicates you in the destruction of the enviornment and the
exploitation of human labor on a global scale&#x2014;I am a strong proponent
of finding sustainable and ethical ways (if there can be any) of
working with this technology.
</p>

<p>
And when it comes to open data, or as is almost always the case in the
context of AI, openly accessible data, that means defining open in a
way that sustains the sharing and mutual cultivation of knowledge
resources.
</p>

<p>
For AI companies, anything that is accessible on a webpage is
considered open to extraction and copying, for the purpose of training
their ever-larger language models. 
</p>

<p>
And right now, in the United States, the question of what data can be
legally harvested by AI companies to use for training their models is
being debated in the courts. There are over 40 ongoing lawsuits
brought by content creators against companies like OpenAI, Anthropic,
Midjourney, and many others, which allege that these companies have
committed copyright infringement by taking openly accessible data and
privatizing the resulting productrs based on that data, without
compensating content creators while simultaneously affecting the
market for their content.
</p>

<p>
To defend themselves, these companies argue that their data harvesting
constitutes "fair use" which allows the taking of copyrighted material
depending on certain conditions. Some of these conditions include the
amount of the work being used, the purpose and nature of the use, the
effect on the market for the original work.
</p>

<p>
SLIDE The New York Times Company v. Microsoft Corporation,
December 27th, 2023 
</p>

<p>
For example, in the lawsuit brought by the NYTimes against Microsoft
(the owner of OpenAI), OpenAI defends itself by saying that data
harvesting is for the purpose of "learning facts" about language,
which is a use that is protected under "fair use."
</p>

<p>
They claim that,
</p>

<p>
SLIDE quote in memorandum of law
</p>

<blockquote>
<p>
&#x2026; it is fair use under copyright law to use publicly accessible
content to train generative AI models to learn about language,
grammar, and syntax, and to understand the facts that constitute
humans’ collective knowledge. OpenAI and the other defendants in these
lawsuits will ultimately prevail because no one—-not even the New York
Times—-gets to monopolize facts or the rules of language.
</p>

<ul class="org-ul">
<li>MEMORANDUM OF LAW filed by OpenAI, February 2024</li>
</ul>
</blockquote>

<p>
So, for this presentation, I'm going to assess this argument from a
technical perspective, to illuminate exactly how training data is
transformed into something like the "facts or rules of language". I'm
going to offer examples from my own research, where I develop ML
models for the purpose of studying social bias, particularly gender
and anti-trans bias, in language. 
</p>

<p>
I'm going to spend some time going over the technical details because,
in my view, these companies gain power and profit from widespread
ignorance about ML and how it actually works in practice. (This
ignorance about ML generally indeed drives much of the marketing and
business adoption).
</p>

<p>
So, by the end I hope I will have left you with some context for
assessing 2 questions&#x2014;first, how do new, computationally-based
language forms differ from our understanding of human-generated
language forms (which is the basis of current copyright law)? and
second: how can we proceed in ways that protect digital language forms
from data colonialism and extraction? 
</p>
</div>
</div>

<div id="outline-container-orgbdc67a9" class="outline-3">
<h3 id="orgbdc67a9"><span class="section-number-3">1.2.</span> the defense of "transformative"</h3>
<div class="outline-text-3" id="text-1-2">
<p>
First, I'm going to examine OpenAI's argumentation more closely, to
trace what exactly they mean by the "facts or rules" of language.
</p>

<p>
Their argumentation begins by appealing to a stipulation from "fair
use" clause, which emphasizes the importance of "transformative-ness",
that is, how much the derivative object <i>changes</i> from the original
object. Here, in a document written to the US Trademark and Patent Office,
OpenAI claim that their product is "highly transformative" 
</p>

<p>
SLIDE authors guild vs hathitrust
</p>

<p>
To demonstrate, they cite a famous legal case, <i>Authors Guild v.
HathiTrust</i>, in 2014, which set a precedent for thinking about the
"transformative" as an aspect distinctly associated with technological
contexts. At the time, Hathitrust had digitized thousands of
copyrighted books into full-text databases for searching.
</p>

<p>
The ruling for this case asserts that the database format of search
results is fundamentally transformative, offering a new object, that
is, <i>information about books</i>, rather than copies of the books
themselves.
</p>

<p>
SLIDE quote from Hathitrust ruling
</p>

<p>
The ruling points out that,
</p>

<blockquote>
<p>
The creation of a full-text searchable database is a quintesssentially
transformative use&#x2026; the result of a word search is different in
purpose, character, expression, meaning, and message from the page
(and the book) from which it is drawn.
</p>

<ul class="org-ul">
<li><i>Author's Guild v. Hathitrust</i>, 2015.</li>
</ul>
</blockquote>

<p>
This ruling, which has significant effect on databases and search
engines, determines that language as <i>aggregation</i>, or an <i>aggregate
form</i>, is fundamentally distinct from langauge in its syntactic
context. As a result of this ruling, activities related to
quantitative analysis, like text mining, become permissable according
to copyright.
</p>

<p>
OpenAI, in turn, applies this understanding of "transformative" to
apply to the language model itself. They argue that, like search
results, language models constitute a new kind of object. But rather
than representing information <i>about</i> the original, this new object
<i>generalizes language patterns</i> from the original.
</p>

<p>
SLIDE OpenAI quote
</p>

<p>
They explain that,
</p>

<blockquote>
<p>
"By learning patterns from its training corpus, an AI system can
eventually generate media that shares some commonalities with works in
the corpus (in the same way that English sentences share some
commonalities with each other by sharing a common grammar and
vocabulary) but cannot be found in it." ("Comment", 9-10)
</p>
</blockquote>

<p>
(I will leave aside for the moment, that while grammar and vocabulary
is not copyrighted, dictionaries and grammar books are indeed
copyrighted material.)
</p>

<p>
What I'm interested in is this use of this word, "patterns," and what
that means in practice. It seems that they are referring to some ideal
of language forms, even a platonic ideal, with regard to grammar and
vocabulary.
</p>

<p>
Now, I want to ask, how does this characterization of language weigh
against these models' operatation in practice, and their outputs?
</p>
</div>
</div>

<div id="outline-container-orgdf85af2" class="outline-3">
<h3 id="orgdf85af2"><span class="section-number-3">1.3.</span> close reading</h3>
<div class="outline-text-3" id="text-1-3">
<p>
To explore this question, I'm going to show a few examples from my
current research, which uses ML tools for the purpose of studying
language forms, using ML as a kind of text analysis tool. For this, I
gather and create custom datasets from various internet sources, which
represent different perspectives. Then, I use these datasets to train
(very small) ML models, and I see how they respond to certain prompts.
The idea is not to generate text for the sake of generating text, but
to surface certain insights about the data on which that text was
trained.
</p>

<p>
For example, I'm going to show some samples of ML-generated text based
on two very different data sources, in fact politically polarized data
sources (from the context of US politics). 
</p>

<p>
SLIDE heritage screenshot
</p>

<p>
One of these sources, representing the conservative pole, is the
Heritage Foundation, which is a think-tank in Washington DC whose goal
is to influence government policy. From their website, I scraped all
the articles that were organized under the topic heading of "gender",
of which you can see some of the headlines here.
</p>

<p>
SLIDE ACLU trans screenshot
</p>

<p>
The second source, from the progressive side, is the ACLU, the
American Civil Liberties Union, which is a group of legal
professionals and volunteers who advocate on behalf of civil rights
for marginalized groups in the US.
</p>

<p>
With these datasets, I then trained two different large language
models, using gpt-2 (an open source model) as the base model. (And if
people are curious about the specifics about this training process, I
am more than happy to answer in the Q&amp;A)
</p>

<p>
Then, after training, I fed a series of prompts to both of the
resulting models. These prompts included:
</p>

<p>
SLIDE prompts
</p>

<blockquote>
<p>
Masculinity is
</p>

<p>
Femininity is
</p>

<p>
Transgender is
</p>

<p>
Transgenderism is
</p>

<p>
Gender binary is
</p>

<p>
Man is
</p>

<p>
Woman is
</p>
</blockquote>

<p>
And finally, I comapared the results.
</p>

<p>
First, there are some obvious contrasts between the model trained on
the ACLU data and model trained on the Heritage data. Here are some
examples from the ACLU results:
</p>

<blockquote>
<p>
Masculinity is a matter of love and celebration.
</p>

<p>
Masculinity is a space for hope and liberation for all.
</p>

<p>
Masculinity is not defined solely by the beauty of our bodies, but by
the beauty of our experiences.
</p>

<p>
Femininity is a celebration of beauty, feminine liberation, and
femininity.
</p>

<p>
Femininity is our joy, our struggle, and our fight is our struggle.
</p>

<p>
Femininity is about allowing people to express themselves without
government interference.
</p>
</blockquote>

<p>
As you can see, for the ACLU model, terms associated with
"masculinity" and "femininity" are characterized by gender-affirming
language, which is very positive and even empowering; using words like
"liberation," "beauty", "celebration," "joy".
</p>

<p>
Reading this, it's not difficult to imagine that these terms
"Masculinity" and "Femininity" may have been prepended by "trans" in
the training data, to be "trans masculinity" and "trans femininity".
</p>

<p>
From these examples, you may also notice the tendency of language
models (especially very small and underdeveloped ones, like mine) to
repeat themselves. This is a fascinating quirk due to their predictive
nature, where the goal is to guess the next word based on what is most
likely. As a result, they get themselves stuck into these little loops
of saying the same thing over and over again.
</p>

<p>
Now, I'll show the text generated by the model trained on the Heritage
Foundation data.
</p>

<blockquote>
<p>
Masculinity is the cornerstone of Western civilization.
</p>

<p>
Masculinity is the fruit of patriarchy, and patriarchy is the heart
of conservatism.
</p>

<p>
Masculinity is defined by the ability to produce sperm, eggs, and live
children.
</p>

<p>
Femininity is an enduring American tradition.
</p>

<p>
Femininity is defined by means of the relationship between the sexes,
the ability to raise their children, the capacity to provide for their
own reproduction, the capacity to provide for their own children, the
ability to provide for their own.
</p>
</blockquote>

<p>
Here, these gender terms are also positive, but their associations are
with culture, tradition, and reproduction, rather than with liberation
and empowerment.
</p>

<p>
So you can see, even from just skimming the results, that there are
direct connections between the training data and the model outputs. By
processing the training data, the model has not just learned how
language works, the "facts or rules" of language, but absorbed the
perspectives contained within that language. So, it is important to
point, that depending on the dataset that the model is trained on, the
terms "masculinity" and "femininity" will have different meanings.
</p>

<p>
What I also found, which complicates this a little bit, are the ways
that these gendered terms reveal certain investments in other terms.
For example, the Heritage Foundation model is highly invested in the
concept of subjectivity, which appears in a lot of its results:
</p>

<blockquote>
<p>
Masculinity is a subjective self-perception, not a universal
concept.
</p>

<p>
Femininity is a subjective, internal sense of self.
</p>

<p>
The gender binary is a subjective, malleable, and often incorrect
idea.
</p>

<p>
The gender binary is a subjective, internal, and often transitory
concept.
</p>

<p>
The gender binary is a subjective, grammatically incorrect and
illogical concept that conflates sex and gender identity.
</p>
</blockquote>

<p>
Reading these, you can see that they do not represent what one would
expect from a typically conservative view&#x2014;which is that gender is
based on biology and therefore, allegedly, verifiable. Rather, they
represent the opposite&#x2014;a progressive view of gender that is based on
personal aspects of identity.
</p>

<p>
The reason for this, I believe, is that this particular term,
"subjective" <i>does not</i> describe the conservative position. Rather, it
describes the conservative view of the progressive position. In other
words, it represents what a transphobic person thinks a progressive
person thinks gender is&#x2014;as some insubstantial, as a feeling. From this
framing, within a conservative worldview, people who do not subscribe
to a biological and binary concept of gender must believe that gender
is "an internal and often transitory" sense of self.
</p>

<p>
This explains why there is a curious hint of derision in some of the
examples, which use terms like "illogical" and "incorrect" alongside
"subjective." These are traces of disqualification and contempt that
survive from the training data.
</p>

<p>
Just to show you, here are some sentences from that training data,
which use the term "subjective":
</p>

<blockquote>
<p>
It’s important to recognize that the notion of ‘gender identity’ is
unscientific, subjective, and political.
</p>

<p>
Why else would there be a need to proselytize these same children into
a world where gender unicorns, drag queens and subjective
self-identification take the place of what their eyes know to be true?
</p>

<p>
Subjective states of mind don’t trump biology.
</p>
</blockquote>

<p>
In the model outputs then, we see not just a single pattern of
language, that is, language that characterizes gender, but a
<i>flattening</i> of patterns of language into a single statement. There
are patterns of distinct expressions, distinct viewpoints, which have
been aggregated into an apparently univocal utterance.
</p>

<p>
Clearly, this language presents a different kind of object from that
which OpenAI claims falls under the protection of "fair use." In
addition to absorbing the "rules and facts" of language, the model
also takes up the perspective of the training data. There is a direct
line between the content that is taken and that which appears in the
generated outputs. And it bears saying that, depending on whose
viewpoints have been absorbed into the language model, this line can
be dangerous for vulnerable groups, like trans people.
</p>

<p>
So, given this situation, I come back to my original question about
sustaining open research, knowledge production, and the sharing of
resources.
</p>

<p>
So, given that these models absorb much more than language forms, but
something like an aggregate of language&#x2014;how can we determine what
kind of use is permissable, in the way of sustaining fair use? 
</p>

<p>
I think the answer has to do with a core concept from data
sovereignty&#x2014;acknowledging fully the data source, which is not only
the creators, the writers in the case of text-based data, but also the
communities that enable the creation of that data.
</p>

<p>
In this regard, there are data initiatives in the global south that
are offering some templates for moving forward.
</p>

<p>
SLIDE nwulite license
</p>

<p>
For example, the Nwulite Obodo license, which was developed by a team
of researchers in Pretoria, South Africa, offers different tiers of
permissions so that users from developing countries can use the model
freely, while other users must either pay or commit to releasing their
derivatives under the same license.
</p>

<p>
SLIDE kaitiakitanga
</p>

<p>
Another licensing initiative comes from the Māori Data Sovereignty
Network, an indigenous group in New Zealand, and is called the
Kaitiakitanga License. This license prohibits commerical use of data,
except in cases where royalties from the use are paid back to the
community, who are the collective stewards of the data.
</p>

<p>
So, moving forward, we have to perpetuate and use these licenses. But
also to understand that language forms do not exist in the air. 
</p>
</div>
</div>
</div>
</div>
<div id="postamble" class="status">
<p class="author">Author: fcalado</p>
<p class="date">Created: 2025-07-11 Fri 17:58</p>
<p class="validation"><a href="https://validator.w3.org/check?uri=referer">Validate</a></p>
</div>
</body>
</html>
